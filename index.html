<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Juergen Mathes - Projects</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body { font-family: system-ui, sans-serif; margin: 0; padding: 2rem; background: #fafafa; color: #222; }
    h1 { margin-bottom: 0.5rem; }
    p  { margin-top: 0; margin-bottom: 2rem; line-height: 1.5; }

    .project-split{
      display:grid; grid-template-columns:1fr 1fr; gap:2rem; align-items:start;
      background:#fff; padding:1.5rem; border-radius:12px; box-shadow:0 2px 8px rgba(0,0,0,0.06);
      margin-bottom:2rem;
    }
    .split-left h2{ margin-top:0; }
    .btn{
      display:inline-block; margin-top:1rem; padding:0.5rem 0.9rem; border:1px solid #ccc;
      border-radius:8px; text-decoration:none; color:#222; font-weight:600;
    }
    .btn:hover{ background:#f5f5f5; }

    .split-right{ display:grid; grid-template-columns:1fr 1fr; gap:1rem; }
    .card{
      margin:0; border:1px solid #eee; border-radius:8px; overflow:hidden; background:#fff; text-align:center;
    }
    .card img {
      width: 100%;
      height: 220px;        /* or whatever height you prefer */
      object-fit: contain;  /* preserve ratio, no skew */
      display: block;
    }
    .same-width {
      width: 100%;
      height: auto;
      object-fit: contain;
    }
    .card figcaption{ padding:0.5rem 0.75rem; font-size:0.9rem; color:#555; border-top:1px solid #eee; }

    /* GIF sizing */
    .gif-img{ width:auto; max-width:100%; max-height:250px; display:inline-block; }

    /* YouTube thumbnail cards */
    .yt-link{
      position:relative; display:block; text-decoration:none; color:inherit;
    }
    .yt-thumb{ display:block; width:100%; height:auto; }
    .yt-play{
      position:absolute; left:50%; top:50%; transform:translate(-50%,-50%);
      background:rgba(0,0,0,0.6); border-radius:999px; padding:12px 16px;
      font-weight:700; color:#fff; line-height:1; user-select:none;
    }
    .yt-link:hover .yt-play{ background:rgba(0,0,0,0.75); }

    /* Responsive */
    @media (max-width:900px){
      .project-split{ grid-template-columns:1fr; }
      .split-right{ grid-template-columns:1fr; }
    }
  </style>
</head>
<body>
  <h1>JÃ¼rgen Mathes</h1>
  <p>Research Engineer bridging state-of-the-art AI and real-world robotics.</p>


  <!-- Tiny PathFormer -->
  <section class="project-split">
    <div class="split-left">
      <h2>Tiny PathFormer</h2>
      <div style="font-size:0.8rem; color:#888; margin-top:-0.3rem; margin-bottom:0.8rem;">
        #playground #fundamentals #weekendproject
      </div>
      <p>
        Lightweight toy project for learning path planning with an autoregressive policy.
        Trains on synthetic grid mazes and visualizes trajectories.
      </p>
      <a class="btn" href="https://github.com/juergenmathes/tiny-pathformer" target="_blank" rel="noopener">View on GitHub</a>
    </div>
    <div class="split-right">
      <figure class="card">
        <img class="gif-img" src="https://github.com/juergenmathes/tiny-pathformer/raw/main/videos/best_ep40_ok.gif" alt="Tiny PathFormer demo ep40">
        <figcaption>Autoregressive rollout (ep40)</figcaption>
      </figure>
      <figure class="card">
        <img class="gif-img" src="https://github.com/juergenmathes/tiny-pathformer/raw/main/videos/best_ep56_ok.gif" alt="Tiny PathFormer demo ep56">
        <figcaption>Autoregressive rollout (ep56)</figcaption>
      </figure>
    </div>
  </section>



  <!-- LineDiffusion / DiffuTraj -->
  <section class="project-split">
    <div class="split-left">
      <h2>LineDiffusion (DiffuTraj)</h2>
      <div style="font-size:0.8rem; color:#888; margin-top:-0.3rem; margin-bottom:0.8rem;">
        #weekendproject #fundamentals
      </div>
      <p>
        Minimal diffusion model for 2D trajectories - from pure noise to straight lines with fixed endpoints.  
        Demonstrates how denoising diffusion can reconstruct trajectories while preserving constraints.  
        Includes dark-themed MP4/GIF animations visualizing the reverse diffusion process.
      </p>
      <a class="btn" href="https://github.com/juergenmathes/LineDiffusion" target="_blank" rel="noopener">View on GitHub</a>
    </div>
    <div class="split-right">
      <figure class="card">
        <img class="gif-img" src="https://github.com/juergenmathes/LineDiffusion/raw/main/outputs/videos/traj_sample_00.gif" alt="LineDiffusion trajectory sample 00">
        <figcaption>Reverse diffusion (sample 00)</figcaption>
      </figure>
      <figure class="card">
        <img class="gif-img" src="https://github.com/juergenmathes/LineDiffusion/raw/main/outputs/videos/traj_sample_01.gif" alt="LineDiffusion trajectory sample 01">
        <figcaption>Reverse diffusion (sample 01)</figcaption>
      </figure>
      <figure class="card">
        <img class="gif-img" src="https://github.com/juergenmathes/LineDiffusion/raw/main/outputs/videos/traj_sample_02.gif" alt="LineDiffusion trajectory sample 02">
        <figcaption>Reverse diffusion (sample 02)</figcaption>
      </figure>
      <figure class="card">
        <img class="gif-img" src="https://github.com/juergenmathes/LineDiffusion/raw/main/outputs/videos/traj_sample_03.gif" alt="LineDiffusion trajectory sample 03">
        <figcaption>Reverse diffusion (sample 03)</figcaption>
      </figure>
    </div>
  </section>



  <!-- S3DPPS Project (local artifacts, clickable to full size) -->
  <section class="project-split">
    <div class="split-left">
      <h2>S3DPPS (3D Pick and Place)</h2>
      <div style="font-size:0.8rem; color:#888; margin-top:-0.3rem; margin-bottom:0.8rem;">
        #weekendproject
      </div>
      <p>
        A straightforward yet powerful approach to <strong>3D bounding box prediction</strong>.<br>
        Starting with pre-segmented object point clouds, a two-stage transformer setup
        <strong>encodes each object</strong> individually and then <strong>cross-attends</strong> across objects
        to refine their 3D bounding boxes.<br>
        The result: an efficient pipeline for <strong>multi-object pick-and-place</strong> with
        headroom for open-world detectors, image fusion, or specialized 3D encoders.
      </p>
      <a class="btn" href="https://github.com/juergenmathes/s3dpps" target="_blank" rel="noopener">View on GitHub</a>
    </div>
    <div class="split-right">
      <figure class="card">
        <a href="artifacts/s3dpps_architecture.png" target="_blank" rel="noopener">
          <img class="gif-img same-width" src="artifacts/s3dpps_architecture.png" alt="S3DPPS final architecture diagram" loading="lazy">
        </a>
        <figcaption>Architecture overview</figcaption>
      </figure>
      <figure class="card">
        <a href="artifacts/s3dpps_streamlit_orig.gif" target="_blank" rel="noopener">
          <img class="gif-img same-width" src="artifacts/s3dpps_streamlit_360.gif" alt="S3DPPS interactive 3D point cloud visualization" loading="lazy">
        </a>
        <figcaption>3D point cloud visualization (click to enlarge)</figcaption>
      </figure>
      <figure class="card">
        <a href="artifacts/s3dpps_3dpreds_orig.gif" target="_blank" rel="noopener">
          <img class="gif-img same-width" src="artifacts/s3dpps_3dpreds_360.gif" alt="S3DPPS final 3D bounding box predictions" loading="lazy">
        </a>
        <figcaption>Final 3D bounding box predictions (click to enlarge)</figcaption>
      </figure>
    </div>
  </section>


   <!-- Video Section: M3DETR (External Presentation) -->
  <section class="project-split">
    <div class="split-left">
      <h2>M3DETR: Multi-representation Multi-scale Transformers for 3D Object Detection</h2>
      <div style="font-size:0.8rem; color:#888; margin-top:-0.3rem; margin-bottom:0.8rem;">
        #paper-presentation
      </div>
      <p>
        Combines raw, voxel, and birdâ€™s-eye-view point cloud representations with multi-scale feature pyramids.  
        M3DETR unifies multiple representations and scales while modeling relationships between point clouds using transformers.  
        Achieves state-of-the-art results on KITTI and Waymo benchmarks.  
        <br><br>
        <strong>Sources:</strong> 
        <a href="https://arxiv.org/abs/2104.11896" target="_blank" rel="noopener">[2104.11896]</a>, 
        <a href="https://arxiv.org/abs/1912.13192" target="_blank" rel="noopener">[1912.13192]</a>
        <br>
        <strong>Tags:</strong> #machinelearning #transformers #objectdetection #pointclouds
      </p>
    </div>
    <div class="split-right">
      <figure class="card">
        <a class="yt-link" href="https://www.youtube.com/watch?v=SQrVUqhrvII" target="_blank" rel="noopener"
           aria-label="Watch M3DETR presentation on YouTube">
          <img class="yt-thumb" src="https://img.youtube.com/vi/SQrVUqhrvII/hqdefault.jpg" alt="M3DETR video thumbnail">
          <span class="yt-play">â–¶</span>
        </a>
        <figcaption>M3DETR video explanation</figcaption>
      </figure>
    </div>
  </section>


  <!-- Video Section: Scene Transformer (External Presentation) -->
  <section class="project-split">
    <div class="split-left">
      <h2>Scene Transformer: Unified Architecture for Multi-Agent Trajectory Prediction</h2>
      <div style="font-size:0.8rem; color:#888; margin-top:-0.3rem; margin-bottom:0.8rem;">
        #paper-presentation
      </div>
      <p>
        Predicts the motion of all agents jointly, producing consistent futures that account for interactions.  
        Inspired by language models with masking strategies for flexible conditioning.  
        Achieves state-of-the-art results on popular autonomous driving datasets.  
        <br><br>
        <strong>Source:</strong> 
        <a href="https://arxiv.org/abs/2106.08417" target="_blank" rel="noopener">[2106.08417]</a>  
        <br>
        <strong>Tags:</strong> #machinelearning #transformers #prediction #planning
      </p>
    </div>
    <div class="split-right">
      <figure class="card">
        <a class="yt-link" href="https://www.youtube.com/watch?v=YJKo_F3uUwU" target="_blank" rel="noopener"
           aria-label="Watch Scene Transformer presentation on YouTube">
          <img class="yt-thumb" src="https://img.youtube.com/vi/YJKo_F3uUwU/hqdefault.jpg" alt="Scene Transformer video thumbnail">
          <span class="yt-play">â–¶</span>
        </a>
        <figcaption>Scene Transformer paper explained</figcaption>
      </figure>
    </div>
  </section>


  <!-- Video Section: End-to-End Video Object Detection (External Presentation) -->
  <section class="project-split">
    <div class="split-left">
      <h2>End-to-End Video Object Detection with Spatial-Temporal Transformers</h2>
      <div style="font-size:0.8rem; color:#888; margin-top:-0.3rem; margin-bottom:0.8rem;">
        #paper-presentation
      </div>
      <p>
        Builds on DETR by introducing Deformable DETR to improve convergence and small-object detection.  
        Uses spatial-temporal transformers for video object detection.  
        Achieves higher performance with 10Ã— fewer epochs.  
        <br><br>
        <strong>Sources:</strong> 
        <a href="https://arxiv.org/abs/2105.10920" target="_blank" rel="noopener">[2105.10920]</a>, 
        <a href="https://arxiv.org/abs/2005.12872" target="_blank" rel="noopener">[2005.12872]</a>, 
        <a href="https://arxiv.org/abs/2010.04159" target="_blank" rel="noopener">[2010.04159]</a>  
        <br>
        <strong>Tags:</strong> #machinelearning #transformers #objectdetection #pointclouds
      </p>
    </div>
    <div class="split-right">
      <figure class="card">
        <a class="yt-link" href="https://www.youtube.com/watch?v=DaULLeq_AAU" target="_blank" rel="noopener"
           aria-label="Watch End-to-End Object Detection presentation on YouTube">
          <img class="yt-thumb" src="https://img.youtube.com/vi/DaULLeq_AAU/hqdefault.jpg" alt="End-to-End Object Detection thumbnail">
          <span class="yt-play">â–¶</span>
        </a>
        <figcaption>Deformable DETR & Video Object Detection</figcaption>
      </figure>
    </div>
  </section>


  <!-- The WayHome Paper (My Publication) -->
  <section class="project-split">
    <div class="split-left">
      <h2>The WayHome: Long-term Motion Prediction on Dynamically Scaled Grids</h2>
      <div style="font-size:0.8rem; color:#888; margin-top:-0.3rem; margin-bottom:0.8rem;">
        #my-publication
      </div>
      <p>
        We develop a novel motion forecasting approach for autonomous vehicles, predicting multiple heatmaps for every nearby traffic participant, one per timestep.  
        A sampling algorithm extracts the most likely coordinates, while a new grid-scaling technique further improves accuracy.  
        Our method reduces miss rates at 3s horizons and remains competitive at longer horizons (up to 8s) on the Waymo Motion Challenge.
      </p>
      <a class="btn" href="https://arxiv.org/abs/2310.04232" target="_blank" rel="noopener">Read on arXiv</a>
    </div>
    <div class="split-right">
      <figure class="card">
        <img class="gif-img same-width" src="artifacts/2023_wayhome_01.png" alt="WayHome artifact 1">
        <figcaption>Fig. 1: Predicted heatmaps per timestep for surrounding traffic participants.</figcaption>
      </figure>
      <figure class="card">
        <img class="gif-img same-width" src="artifacts/2023_wayhome_02.png" alt="WayHome artifact 2">
        <figcaption>Fig. 2: Neural network model with encoders, decoders, and grid-scaling technique.</figcaption>
      </figure>
    </div>
  </section>


  <!-- Scaling Planning Paper (My Publication) -->
  <section class="project-split">
    <div class="split-left">
      <h2>Scaling Planning for Automated Driving using Simplistic Synthetic Data</h2>
      <div style="font-size:0.8rem; color:#888; margin-top:-0.3rem; margin-bottom:0.8rem;">
        #my-publication
      </div>
      <p>
        We challenge the view that deep learning for driving plans needs massive real-world datasets or highly realistic simulations.  
        With lightweight synthetic roundabout scenarios and behavioural cloning, we achieve reliable and comfortable driving in a real vehicle, while addressing sim-to-real gaps through targeted augmentation and scenario variations.
      </p>
      <a class="btn" href="https://arxiv.org/abs/2305.18942" target="_blank" rel="noopener">Read on arXiv</a>
    </div>
    <div class="split-right">
      <figure class="card">
        <img class="gif-img" src="artifacts/2023_scaling_imitation_01.png" alt="Scaling Planning artifact 1">
        <figcaption>
          Fig. 1: Alternating steps of our iterative development cycle:  
          train and optimise planner with simplistic simulation (left),  
          observe sim-to-real gap in on-road test and extend simulator (right).
        </figcaption>
      </figure>
      <figure class="card">
        <img class="gif-img" src="artifacts/2023_scaling_imitation_02.png" alt="Scaling Planning artifact 2">
        <figcaption>
          Fig. 2: Model architecture consisting of a CNN backbone,  
          a waypoints head, and a prediction head that generates the  
          auxiliary prediction output.
        </figcaption>
      </figure>
    </div>
  </section>
 



  <!-- Patents & Links -->
  <section class="project-split" style="display:block">
    <h2>Patents</h2>
    <ul style="list-style:none; padding:0; margin:0; line-height:1.8;">
      <li>ðŸ’¡ <a href="https://patents.google.com/patent/US20250021879A1/en" target="_blank" rel="noopener">
        <strong>Computer-Implemented Method and System for Training a Planning Model</strong></a> (2025)</li>
      <li>ðŸ’¡ <a href="https://patents.google.com/patent/US20240278808A1/en" target="_blank" rel="noopener">
        <strong>Method for Behavior Planning of an Ego Vehicle as Part of a Traffic Scene</strong></a> (2024)</li>
    </ul>

    <h2 style="margin-top:1.5rem;">Links</h2>
    <ul style="list-style:none; padding:0; margin:0; line-height:1.8;">
      <li>ðŸ”— <a href="https://www.linkedin.com/in/juergen-mathes-44b389131/" target="_blank" rel="noopener">
        LinkedIn â€“ professional profile</a></li>
      <li>ðŸ”— <a href="https://scholar.google.com/citations?user=6qs34YkAAAAJ&hl=en&oi=ao" target="_blank" rel="noopener">
        Google Scholar â€“ list of publications</a></li>
    </ul>
  </section>


</body>
</html>
